{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function for each row of the input x.\n",
    "\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code.\n",
    "    You might find numpy functions np.exp, np.sum, np.reshape,\n",
    "    np.max, and numpy broadcasting useful for this task. (numpy\n",
    "    broadcasting documentation:\n",
    "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
    "\n",
    "    You should also make sure that your code works for one\n",
    "    dimensional inputs (treat the vector as a row), you might find\n",
    "    it helpful for your later problems.\n",
    "\n",
    "    You must implement the optimization in problem 1(a) of the \n",
    "    written assignment!\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE \n",
    "    ### 根据softmax(x) = softmax(x+c) 施行x-np.max(x)后再exp防止溢出\n",
    "    \n",
    "    if len(x.shape) == 1 :\n",
    "        exp = np.exp(x - np.max(x))\n",
    "        x = exp / np.sum(exp)\n",
    "    else:\n",
    "        x_max = x.max(axis=1)\n",
    "        if len(x_max.shape) == 1:\n",
    "            x_max = x_max.reshape(x_max.shape[0],1)\n",
    "        exp = np.exp(x - x_max)\n",
    "        exp_sum = exp.sum(axis=1)\n",
    "        if len(exp_sum.shape) == 1:\n",
    "            exp_sum = exp_sum.reshape(exp_sum.shape[0],1)\n",
    "        x = exp / exp_sum\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started. \n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print(\"Running basic tests...\")\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print(test1)\n",
    "    assert np.amax(np.fabs(test1 - np.array(\n",
    "        [0.26894142,  0.73105858]))) <= 1e-6\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print(test2)\n",
    "    assert np.amax(np.fabs(test2 - np.array(\n",
    "        [[0.26894142, 0.73105858], [0.26894142, 0.73105858]]))) <= 1e-6\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print(test3)\n",
    "    assert np.amax(np.fabs(test3 - np.array(\n",
    "        [0.73105858, 0.26894142]))) <= 1e-6\n",
    "\n",
    "    print(\"You should verify these results!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_softmax():\n",
    "    \"\"\" \n",
    "    Use this space to test your softmax implementation by running:\n",
    "        python q1_softmax.py \n",
    "    This function will not be called by the autograder, nor will\n",
    "    your tests be graded.\n",
    "    \"\"\"\n",
    "    print(\"Running your tests...\")\n",
    "    ### YOUR CODE HERE\n",
    "    x = np.array([[2.5,2.5,3.3],[745.6,586.2,635.3],[9987.2,5243.1,6584.1],[0.254,0.364,0.894],[12.54,784.3,125.2]])\n",
    "    result = softmax(x)\n",
    "    print(result)\n",
    "    ### END YOUR CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[ 0.26894142  0.73105858]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n",
      "You should verify these results!\n",
      "\n",
      "Running your tests...\n",
      "[[  2.36656091e-001   2.36656091e-001   5.26687817e-001]\n",
      " [  1.00000000e+000   5.93553110e-070   1.25117669e-048]\n",
      " [  1.00000000e+000   0.00000000e+000   0.00000000e+000]\n",
      " [  2.49205101e-001   2.78182189e-001   4.72612709e-001]\n",
      " [  0.00000000e+000   1.00000000e+000   5.70830247e-287]]\n"
     ]
    }
   ],
   "source": [
    "test_softmax_basic()\n",
    "test_softmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
